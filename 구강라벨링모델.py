# -*- coding: utf-8 -*-
"""구강라벨링모델.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13u_lxDThl6F5suw6ISsC_LdYD74RB51z
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import glob
from tqdm import tqdm
import json
import numpy as np
import sys

# ================================================================= #
# 경로 설정 (자네가 준 좌표 적용 완료)
# ================================================================= #
JSON_REFERENCE_DIR = '/content/drive/MyDrive/모프 구강 데이터/front'
ORIGINAL_IMAGE_DIR = '/content/drive/MyDrive/3.개방데이터/1.데이터/Training/1.원천데이터/2.front'
MASK_IMAGE_DIR = '/content/drive/MyDrive/모프/탐지모델_데이터/마스크데이터_front'

# 학습 설정 (512 해상도 / 배치 4)
IMAGE_SIZE = (512, 512)
BATCH_SIZE = 4
EPOCHS = 50
LEARNING_RATE = 1e-4
NUM_TRAINING_FILES = 16000

# ================================================================= #
# Dataset 클래스 (수정 불필요)
# ================================================================= #
class ToothSegmentationDataset(Dataset):
    def __init__(self, original_image_dir, mask_image_dir, json_dir, num_files=None, augment=False, img_size=(512, 512)):
        self.file_pairs = []
        self.img_size = img_size
        self.augment = augment

        print(f">>> 데이터 매칭 시작... (원본: {original_image_dir})")

        # JSON 파일 목록 가져오기
        all_json_files = sorted(glob.glob(os.path.join(json_dir, '*.json')))

        # 개수 제한
        if num_files:
            all_json_files = all_json_files[:min(num_files, len(all_json_files))]

        # 파일 쌍 매칭
        for json_path in all_json_files:
            base_name = os.path.splitext(os.path.basename(json_path))[0]
            img_path = os.path.join(original_image_dir, f"{base_name}.png")
            mask_path = os.path.join(mask_image_dir, f"{base_name}.png") # 1단계에서 생성된 파일

            # 파일 존재 여부 확인
            if os.path.exists(img_path) and os.path.exists(mask_path):
                self.file_pairs.append({'image': img_path, 'mask': mask_path})

        if not self.file_pairs:
            print("!!! [치명적 오류] 매칭되는 데이터가 하나도 없소!")
            print(f"확인 1: {original_image_dir} 에 이미지가 있는가?")
            print(f"확인 2: {mask_image_dir} 에 마스크가 생성되었는가? (1단계 실행 필수)")
            raise ValueError("데이터 매칭 실패")

        print(f">>> 총 {len(self.file_pairs)}쌍의 데이터 준비 완료!")

        # 변환 정의
        if self.augment:
            self.transform = transforms.Compose([
                transforms.Resize(self.img_size),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(10),
                transforms.ColorJitter(brightness=0.2, contrast=0.2),
                transforms.ToTensor()
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize(self.img_size),
                transforms.ToTensor()
            ])

    def __len__(self): return len(self.file_pairs)

    def __getitem__(self, idx):
        pair = self.file_pairs[idx]
        image = Image.open(pair['image']).convert("RGB")
        mask = Image.open(pair['mask']).convert("L")

        seed = np.random.randint(2147483647)
        torch.manual_seed(seed)
        image = self.transform(image)

        torch.manual_seed(seed)
        mask_trans = transforms.Compose([t for t in self.transform.transforms if not isinstance(t, transforms.ColorJitter)])
        mask = mask_trans(mask)

        mask = (mask > 0.5).float()
        return image, mask

# ================================================================= #
# U-Net 모델 & 학습 루프
# ================================================================= #
# (모델 클래스 생략 - 위와 동일함, 실행 시 자동으로 정의됨을 가정하거나 필요시 아까 코드 복사)
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__(); self.double_conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))
    def forward(self, x): return self.double_conv(x)
class Down(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__(); self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))
    def forward(self, x): return self.maxpool_conv(x)
class Up(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__(); self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2); self.conv = DoubleConv(in_channels, out_channels)
    def forward(self, x1, x2):
        x1 = self.up(x1); diffY = x2.size()[2] - x1.size()[2]; diffX = x2.size()[3] - x1.size()[3]; x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2]); x = torch.cat([x2, x1], dim=1); return self.conv(x)
class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__(); self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
    def forward(self, x): return self.conv(x)
class UNet(nn.Module):
    def __init__(self, n_channels=3, n_classes=1):
        super(UNet, self).__init__(); self.inc = DoubleConv(n_channels, 64); self.down1 = Down(64, 128); self.down2 = Down(128, 256); self.down3 = Down(256, 512); self.down4 = Down(512, 1024); self.up1 = Up(1024, 512); self.up2 = Up(512, 256); self.up3 = Up(256, 128); self.up4 = Up(128, 64); self.outc = OutConv(64, n_classes)
    def forward(self, x):
        x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2); x4 = self.down3(x3); x5 = self.down4(x4); x = self.up1(x5, x4); x = self.up2(x, x3); x = self.up3(x, x2); x = self.up4(x, x1); logits = self.outc(x); return logits

# 실행
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n>>> 작전 장비: {device} | 해상도: {IMAGE_SIZE} | 배치: {BATCH_SIZE}")

full_dataset = ToothSegmentationDataset(ORIGINAL_IMAGE_DIR, MASK_IMAGE_DIR, JSON_REFERENCE_DIR, num_files=NUM_TRAINING_FILES, augment=True, img_size=IMAGE_SIZE)

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

model = UNet().to(device)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.BCEWithLogitsLoss()

print(">>> 훈련 시작!")
SAVE_PATH = '/content/drive/MyDrive/모프/학습모델/치아좌표탐지모델_unet_front_512.pth'
best_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    loop = tqdm(train_loader, desc=f"Ep {epoch+1}/{EPOCHS}", leave=False)
    epoch_loss = 0
    for img, mask in loop:
        img, mask = img.to(device), mask.to(device)
        pred = model(img)
        loss = criterion(pred, mask)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        loop.set_postfix(loss=loss.item())

    # 간단 검증 및 저장
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for img, mask in val_loader:
            img, mask = img.to(device), mask.to(device)
            pred = model(img)
            val_loss += criterion(pred, mask).item()

    val_loss /= len(val_loader)
    print(f"Ep {epoch+1} done. Train Loss: {epoch_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}")

    if val_loss < best_loss:
        best_loss = val_loss
        torch.save(model.state_dict(), SAVE_PATH)
        print("  --> 모델 저장됨!")